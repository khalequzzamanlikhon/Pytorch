{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "The autograd package provides automatic differentiation \n",
    " for all operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3134, 0.5469, 0.4309], requires_grad=True)\n",
      "tensor([2.3134, 2.5469, 2.4309], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000001E8507957E0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#requires_grad =True-> tracks all operations on the operator\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "y=x+2\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x)# created by the user-> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.0550, 19.4599, 17.7282], grad_fn=<MulBackward0>)\n",
      "tensor(17.7477, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#do more operations on y\n",
    "z=y*y*3\n",
    "print(z)\n",
    "z=z.mean()\n",
    "print(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the gradients with backpropagation.\n",
    "When we finish our computation we can call backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into grad attribute.\n",
    "It is the partial derivate of the function w.r.t. the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.6267, 5.0938, 4.8618])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, torch.autograd is an engine for computing vector-Jacobian product.\n",
    "It computes partial derivates while applying the chain rule.\n",
    " \n",
    " Model with non-scalar output:\n",
    " If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() .\n",
    " specify a gradient argument that is a tensor of matching shape.\n",
    " needed for vector-Jacobian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -662.0009,  2165.6428, -1005.0162], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,requires_grad=True)\n",
    "y=x*2\n",
    "for _ in range(10):\n",
    "    y*=2\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "v=torch.tensor([0.1,1.0,0.0001],dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Stop a tensor from tracking history:\n",
    " For example during our training loop when we want to update our weights\n",
    " then this update operation should not be part of the gradient computation\n",
    "- x.requires_grad_(False)\n",
    " - x.detach()\n",
    "- wrap in 'with torch.no_grad():'\n",
    "\n",
    ".requires_grad_(...) changes an existing flag in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x000001E850795AB0>\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(2,2)\n",
    "print(a.requires_grad)\n",
    "b=((a*3)/(a-1))\n",
    "print(b.grad_fn)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b=(a*a).sum()\n",
    "print(b.grad_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### .detach(): get a new Tensor with the same content but no gradient computation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(2,2,requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "b=a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrap in 'with torch.no_grad():'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(2,2,requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "!!! We need to be careful during optimization !!!\n",
    "Use .zero_() to empty the gradients before a new optimization step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "tensor(4.8000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(model_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimizer has zero_grad() method\n",
    "- optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "##### During training:\n",
    "- optimizer.step()\n",
    "- optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
