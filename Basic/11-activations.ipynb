{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "without activation fucntion we get only linear funtion. But sometimes linear function does not fit well when considering non linear or more complex examples. In this situation activation functions comes into consideration. There are several types of activation function.All are available in pytorch. they are -> \n",
    "\n",
    "- Step  :\n",
    "    - outputs 1 if x>= threshold and 0 otherwise. not used in practice\n",
    "- Sigmoid :\n",
    "    - typically in the last layer of a binary classification problem\n",
    "- TanH :\n",
    "    - scaled sigmoid funcion with little bit shift. used in hidden layer\n",
    "- ReLU  :\n",
    "    - most popular choice. outputs 0 for negative values and for output for positive value.\n",
    "    - if you dont know what to use then use ReLU\n",
    "    - f(x)=max(0,x)\n",
    "\n",
    "- Leaky ReLU :\n",
    "     - slightly improved version of ReLU. x if x>=0 and a*x otherwise.\n",
    "     - trying to solve the vanishing gradient problem\n",
    "- softmax :\n",
    "    -good in last layer in multiclass problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as f \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output = w*x + b\n",
    "- output = activation_function(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.0, 1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0120, 0.0889, 0.2418, 0.6572])\n",
      "tensor([0.0120, 0.0889, 0.2418, 0.6572])\n"
     ]
    }
   ],
   "source": [
    "# sofmax\n",
    "output = torch.softmax(x, dim=0)\n",
    "print(output)\n",
    "sm = nn.Softmax(dim=0)\n",
    "output = sm(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2689, 0.7311, 0.8808, 0.9526])\n",
      "tensor([0.2689, 0.7311, 0.8808, 0.9526])\n"
     ]
    }
   ],
   "source": [
    "# sigmoid \n",
    "output = torch.sigmoid(x)\n",
    "print(output)\n",
    "s = nn.Sigmoid()\n",
    "output = s(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7616,  0.7616,  0.9640,  0.9951])\n",
      "tensor([-0.7616,  0.7616,  0.9640,  0.9951])\n"
     ]
    }
   ],
   "source": [
    "#tanh\n",
    "output = torch.tanh(x)\n",
    "print(output)\n",
    "t = nn.Tanh()\n",
    "output = t(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# relu\n",
    "output = torch.relu(x)\n",
    "print(output)\n",
    "relu = nn.ReLU()\n",
    "output = relu(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0100,  1.0000,  2.0000,  3.0000])\n",
      "tensor([-0.0100,  1.0000,  2.0000,  3.0000])\n"
     ]
    }
   ],
   "source": [
    "# leaky relu\n",
    "output = f.leaky_relu(x)\n",
    "print(output)\n",
    "lrelu = nn.LeakyReLU()\n",
    "output = lrelu(x)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "nn.ReLU() creates an nn.Module which you can add e.g. to an nn.Sequential model.\n",
    "torch.relu on the other side is just the functional API call to the relu function,\n",
    "so that you can add it e.g. in your forward method yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.linear1=nn.Linear(input_size,hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(hidden_size,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        out=self.linear1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.linear2(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2 (use activation function directlyl in forward pass)\n",
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
